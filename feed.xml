<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://19revey.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://19revey.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-29T17:55:13+00:00</updated><id>https://19revey.github.io/feed.xml</id><title type="html">Yifei Duan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Physics Informed Neural Network</title><link href="https://19revey.github.io/blog/2024/pinn/" rel="alternate" type="text/html" title="Physics Informed Neural Network"/><published>2024-03-20T12:57:00+00:00</published><updated>2024-03-20T12:57:00+00:00</updated><id>https://19revey.github.io/blog/2024/pinn</id><content type="html" xml:base="https://19revey.github.io/blog/2024/pinn/"><![CDATA[ <h2 style="color:purple;font-size: 2em;">Overview</h2> <ul> <li><a href="#section1">1. Introduction</a></li> <li><a href="#section2">2. Neural Netowork</a></li> <li><a href="#section3">3. Model Training</a></li> <li><a href="#section4">4. Results</a></li> <li><a href="#section5">5. Improvements</a></li> </ul> <h5 id="links-to-run-the-code">Links to run the code</h5> <ul> <li><a href="https://colab.research.google.com/github/19revey/PINN_granular_segregation/blob/main/notebook/solver_segregation.ipynb">Google colab</a></li> <li><a href="https://github.com/19revey/PINN_granular_segregation">Github repo</a></li> </ul> <h5 id="understand-the-granular-segregation-and-the-transport-equation">Understand the granular segregation and the transport equation</h5> <ul> <li><a href="https://arxiv.org/pdf/2309.13273.pdf">[1] General Model For Segregation Forces in Flowing Granular Mixtures</a></li> <li><a href="https://arxiv.org/pdf/1809.08089.pdf">[2] Diffusion, mixing, and segregation in confined granular flows</a></li> <li><a href="https://pubs.acs.org/doi/10.1021/acs.iecr.5b01268">[3] On Mixing and Segregation: From Fluids and Maps to Granular Solids and Advection–Diffusion Systems</a></li> </ul> <h5 id="pinn-implementation">PINN implementation</h5> <ul> <li><a href="https://github.com/nanditadoloi/PINN">https://github.com/nanditadoloi/PINN</a></li> <li><a href="https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks">https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks</a></li> <li><a href="https://github.com/maziarraissi/PINNs">https://github.com/maziarraissi/PINNs</a></li> </ul> <p>Here is a li</p> <p><a class="anchor" id="section1"></a></p> <h2 style="color:purple;font-size: 2em;">1. Introduction</h2> <p>An advection-diffusion transport equation has been successfully used to model the segregation. Within this continuum framework, the concentration of species \(i\) can be expressed as</p> \[\frac{\partial c_i}{\partial t} + {\nabla \cdot (\pmb u c_i)}={\nabla \cdot (D\nabla c_i)}.\] <p>With assumption of incompressible flow and negligible vertical acceleration, the above equation in the \(z\) direction can be written as</p> \[\frac{\partial c_i}{\partial t} +\frac{\partial (w+w_{i})c_i}{\partial z}=\frac{\partial}{\partial z} \Big( D\frac{\partial c_i}{\partial z} \Big),\] <p>or, rearranging, as</p> \[w_{i}c_i-D\frac{\partial c_i}{\partial z}=0,\] <p>where \(w_{i}\) is the segregation velocity relative to the bulk velocity \(w\).</p> <table> <thead> <tr> <th> </th> <th> </th> <th>full model</th> <th>simplified model</th> </tr> </thead> <tbody> <tr> <td>intruder scaled segregation force</td> <td>\(F_{i,0}\)</td> <td>\(-f^g(R)\frac{\partial{p}}{\partial{z}}V_i+f^k(R)\frac{p}{\dot\gamma}\frac{\partial\dot\gamma}{\partial{z}}V_i\)</td> <td> </td> </tr> <tr> <td>Mixture scaled segregation force</td> <td>\(\hat F_{l}\) <br/> \(\hat{F}_{s}\)</td> <td>\((\hat{F}_{l,0}-\cos{\theta})\textrm{tanh}\Big( \frac{\cos{\theta}-\hat{F}_{s,0}}{\hat{F}_{l,0}-\cos{\theta}}\frac{c_s}{c_l} \Big)\) <br/> \(-(\hat{F}_{l,0}-\cos{\theta}){\frac{c_l}{c_s}}\textrm{tanh}\Big( \frac{\cos{\theta}-\hat{F}_{s,0}}{\hat{F}_{l,0}-\cos{\theta}}\frac{c_s}{c_l} \Big)\)</td> <td> </td> </tr> <tr> <td>effective friction</td> <td>\(\mu_{eff}\)</td> <td>\(\mu_s+\frac{\mu_2-\mu_s}{I_c/I+1}\)</td> <td> </td> </tr> <tr> <td>viscosity</td> <td>\(\eta\)</td> <td>\(\mu_{eff} \frac{P}{\dot\gamma}\)</td> <td> </td> </tr> <tr> <td>drag coefficient</td> <td>\(c_{d,l}\) <br/> \(c_{d,s}\)</td> <td>\([k_1-k_2\exp(-k_3 R)]+s_1 I R +s_2 I (R_\rho-1)\) <br/> \(c_{d,l}/R^2\)</td> <td> </td> </tr> <tr> <td>segregation velocity</td> <td>\(w_l\) <br/> \(w_s\)</td> <td>\(\frac{ \hat F_{l} m_l g_0}{c_{d,l} \pi \eta d_l}\) <br/> \(-\frac{ \hat F_s m_s g_0}{c_{d,s} \pi \eta d_s}\)</td> <td>\(0.26 d_s \ln R \dot\gamma (1-c_i)\)</td> </tr> <tr> <td>diffusion coefficient</td> <td>\(D\)</td> <td>\(0.042 \dot \gamma (c_ld_l+c_sd_s)^2\)</td> <td>\(0.042 \dot \gamma {\bar d}^2\)</td> </tr> </tbody> </table> <p><a class="anchor" id="section2"></a></p> <h2 style="color:purple;font-size: 2em;">2. Physics Informed Neural Network</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">mps</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>

        <span class="c1"># 6 layer neural network
</span>        <span class="nf">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer5</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mse</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>
        
        <span class="c1"># particle properties in S.I unit
</span>        <span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="o">=</span><span class="mi">2</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dl</span><span class="o">=</span><span class="mf">0.004</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="o">=</span><span class="mi">1000</span>
        <span class="n">self</span><span class="p">.</span><span class="n">c_diffusion</span><span class="o">=</span><span class="mf">0.042</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ds</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dl</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rds</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">=</span><span class="mi">4</span><span class="o">/</span><span class="mi">3</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="mf">0.002</span><span class="o">**</span><span class="mi">3</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ms</span><span class="o">=</span><span class="mi">4</span><span class="o">/</span><span class="mi">3</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="mf">0.001</span><span class="o">**</span><span class="mi">3</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span>
        
        
        <span class="c1"># flow configuration (uniform shear)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span><span class="o">=</span><span class="mi">100</span>
        <span class="n">self</span><span class="p">.</span><span class="n">phi</span><span class="o">=</span><span class="mf">0.55</span>
        <span class="n">self</span><span class="p">.</span><span class="n">g</span><span class="o">=</span><span class="mf">9.81</span>
        <span class="n">self</span><span class="p">.</span><span class="n">h0</span><span class="o">=</span><span class="mf">0.01</span>
        <span class="n">self</span><span class="p">.</span><span class="n">p0</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">h0</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">g</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">phi</span>

        <span class="c1"># segregation force calculation
</span>        <span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  
        <span class="c1">#  Duan et al. 2024
</span>        <span class="n">_intruder_l</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mf">1.43</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="o">/</span><span class="mf">0.92</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mf">3.55</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="o">/</span><span class="mf">2.94</span><span class="p">))</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">phi</span>
        <span class="n">_intruder_s</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mf">1.43</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">rds</span><span class="o">/</span><span class="mf">0.92</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mf">3.55</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">rds</span><span class="o">/</span><span class="mf">2.94</span><span class="p">))</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">phi</span>

        <span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">_intruder_l</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">intruder_s</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">_intruder_s</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="c1"># if time dimensino is considered, concatenated first, i.e. torch.cat([z,t],axis=1) 
</span>        <span class="n">layer1_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln1</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer1</span><span class="p">(</span><span class="n">z</span><span class="p">)))</span>
        <span class="n">layer2_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln2</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer2</span><span class="p">(</span><span class="n">layer1_out</span><span class="p">)))</span>
        <span class="n">layer3_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln3</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer3</span><span class="p">(</span><span class="n">layer2_out</span><span class="p">)))</span>
        <span class="n">layer4_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln4</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer4</span><span class="p">(</span><span class="n">layer3_out</span><span class="p">)))</span>
        <span class="n">layer5_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer5</span><span class="p">(</span><span class="n">layer4_out</span><span class="p">))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">output_layer</span><span class="p">(</span><span class="n">layer5_out</span><span class="p">)</span> <span class="c1">## For regression, no activation is used in output layer
</span>        <span class="k">return</span> <span class="n">output</span>
    
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>


        <span class="c1"># PDE loss    
</span>        <span class="n">z_collocation</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10001</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">z_collocation</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 

        <span class="n">c</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> 
        
        <span class="n">p</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">phi</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">g</span><span class="o">*</span><span class="n">z</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">p0</span>
        <span class="n">inert</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">gamma</span><span class="o">*</span><span class="p">(</span><span class="n">c</span><span class="o">*</span><span class="mf">0.004</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">0.004</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">p</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="p">);</span>
        <span class="n">mu_eff</span><span class="o">=</span><span class="mf">0.364</span><span class="o">+</span><span class="p">(</span><span class="mf">0.772</span><span class="o">-</span><span class="mf">0.364</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mf">0.434</span><span class="o">/</span><span class="n">inert</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">eta</span><span class="o">=</span><span class="n">mu_eff</span><span class="o">*</span><span class="n">p</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">gamma</span>

        <span class="n">mixture_l</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_s</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">ms</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">/</span><span class="n">c</span><span class="p">)</span>
        <span class="n">mixture_s</span><span class="o">=-</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">c</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ms</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_s</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">ms</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">/</span><span class="n">c</span><span class="p">)</span>
        

        <span class="n">cd</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">2.6</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="p">))</span><span class="o">+</span><span class="mf">0.57</span><span class="o">*</span><span class="n">inert</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span>

        <span class="n">wseg</span><span class="o">=</span><span class="n">mixture_l</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">g</span> <span class="o">/</span> <span class="p">(</span><span class="n">cd</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">eta</span><span class="o">*</span><span class="mf">0.004</span><span class="p">)</span>

        <span class="n">c_z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="nf">sum</span><span class="p">(),</span> <span class="n">z</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Duan et al. 2024  
</span>        <span class="n">pde</span><span class="o">=</span><span class="p">(</span><span class="n">wseg</span><span class="o">*</span><span class="n">c</span><span class="o">-</span><span class="mf">0.042</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">square</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ds</span><span class="o">+</span><span class="n">c</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">dl</span><span class="p">)</span><span class="o">*</span><span class="n">c_z</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>

        <span class="c1"># Schlick et al. 2015
</span>        <span class="c1"># simplified with constant diffusion coefficient
</span>        <span class="c1"># pde = (1/0.1 *(1-c)*c - c_z )*10
</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">pde</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">pde_loss</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">mse</span><span class="p">(</span><span class="n">pde</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>


        <span class="c1"># Mass conservation loss
</span>        <span class="n">x_bc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">10001</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">x_bc</span> <span class="o">=</span> <span class="n">x_bc</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">u_bc</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">x_bc</span><span class="p">)</span>
        <span class="n">u_bc</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">u_bc</span><span class="p">)</span>
        
        <span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">u_bc</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">+</span><span class="mf">0.5</span>
        <span class="n">mass_loss</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">mse</span><span class="p">(</span><span class="n">u_bc</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>
   
        <span class="k">return</span> <span class="n">mass_loss</span>  <span class="o">+</span> <span class="n">pde_loss</span>
</code></pre></div></div> <p><a class="anchor" id="section3"></a></p> <h2 style="color:purple;font-size: 2em;">3. Train </h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">mse_cost_function</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span> <span class="c1"># Mean squared error
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.002</span><span class="p">)</span>


<span class="n">iterations</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">previous_validation_loss</span> <span class="o">=</span> <span class="mf">99999999.0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span> <span class="c1"># to make the gradients zero
</span>    <span class="n">loss</span><span class="o">=</span><span class="n">net</span><span class="p">.</span><span class="nf">loss</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span> <span class="c1"># This is for computing gradients using backward propagation
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span> <span class="c1"># This is equivalent to : theta_new = theta_old - alpha * derivative of J w.r.t theta
</span>    <span class="k">if</span> <span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="mf">1e-6</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    	<span class="nf">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span><span class="sh">"</span><span class="s">Traning Loss:</span><span class="sh">"</span><span class="p">,</span><span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></div> <p><a class="anchor" id="section4"></a></p> <h2 style="color:purple;font-size: 2em;">4. Results </h2> <p>The full model prediction is compared to the previous model by Schlick et al. 2015 with different values of \(\lambda\).</p> \[\frac{d_c}{d_z}=\frac{1}{\lambda}c_l(1-c_l)\] <h2 style="color:purple;font-size: 1em;">4.1 Uniform shear </h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison001-480.webp 480w,/assets/img/blog_img/comparison001-800.webp 800w,/assets/img/blog_img/comparison001-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison01-480.webp 480w,/assets/img/blog_img/comparison01-800.webp 800w,/assets/img/blog_img/comparison01-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison01.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 style="color:purple;font-size: 1em;">4.2 Exponential shear </h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison001exp-480.webp 480w,/assets/img/blog_img/comparison001exp-800.webp 800w,/assets/img/blog_img/comparison001exp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison001exp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison01exp-480.webp 480w,/assets/img/blog_img/comparison01exp-800.webp 800w,/assets/img/blog_img/comparison01exp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison01exp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a class="anchor" id="section5"></a></p> <h2 style="color:purple;font-size: 2em;">5. Improvements </h2> <ul> <li>Add loss function for \(c\) out of range 0 to 1</li> <li>Add a learning rate scheduler to reduce learning rate at loss plateau (lr reduced from 1e-3 to 1e-5)</li> </ul> <p>After these changes, the overall loss is reduced to 1e-8</p> <h2 style="color:purple;font-size: 1em;">5.1 Exponential shear </h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison001exp_a-480.webp 480w,/assets/img/blog_img/comparison001exp_a-800.webp 800w,/assets/img/blog_img/comparison001exp_a-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison001exp_a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison01exp_a-480.webp 480w,/assets/img/blog_img/comparison01exp_a-800.webp 800w,/assets/img/blog_img/comparison01exp_a-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison01exp_a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[PINN for solving segregation problem]]></summary></entry><entry><title type="html">Transformer from scratch using PyTorch</title><link href="https://19revey.github.io/blog/2023/transformer/" rel="alternate" type="text/html" title="Transformer from scratch using PyTorch"/><published>2023-07-11T12:57:00+00:00</published><updated>2023-07-11T12:57:00+00:00</updated><id>https://19revey.github.io/blog/2023/transformer</id><content type="html" xml:base="https://19revey.github.io/blog/2023/transformer/"><![CDATA[ <h2 style="color:purple;font-size: 2em;">Overview</h2> <p>Basic transformer with an encoder-decoder architecture for language translation models.</p> <ul> <li>Understanding transformers <ul> <li><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">attention is all you need</a></li> </ul> </li> <li>Pytorch implementation <ul> <li><a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html">https://pytorch.org/tutorials/beginner/transformer_tutorial.html</a></li> <li><a href="https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch">https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch</a></li> </ul> </li> </ul> <p><a class="anchor" id="section1"></a></p> <h2 style="color:purple;font-size: 2em;">1. Introduction</h2> <p><img src="https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png" width="330" height="470" img="" style="float: right;"/></p> <ul> <li><a href="#section1">1. Introduction</a></li> <li><a href="#section2">2. Import libraries</a></li> <li><a href="#section3">3. Basic components</a> <ul> <li><a href="#section4">Word Embeddings</a></li> <li><a href="#section5">Positional Encoding</a></li> <li><a href="#section6">Self Attention</a></li> <li><a href="#section7">Transformer Block</a></li> </ul> </li> <li><a href="#section8">4. Encoder</a></li> <li><a href="#section9">5. Decoder</a></li> <li><a href="#section10">6. Transformer</a></li> </ul> <p><a class="anchor" id="section2"></a></p> <h2 style="color:purple;font-size: 2em;">2. Import Libraries</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">mps</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div> <p><a class="anchor" id="section3"></a></p> <h2 style="color:purple;font-size: 2em;">3. Basic components</h2> <p><a class="anchor" id="section4"></a></p> <h2 style="color:purple;font-size: 1.5em;">Word Embeddings</h2> <p>Each word will be mapped to corresponding \(d_{model}=512\) embedding vector. Suppose we have batch_size of 32 and sequence_length of 10 (10 words). The the output will be Batch_size X sequence_length X embedding_dim (32X10X512).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            vocab_size: size of vocabulary
            embed_dim: dimension of embeddings, i.e. d_model
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            x: input vector, i.e. (batch, seq_len, vocab_size)
        Returns:
            out: embedding vector (batch, seq_len, embed_dim)
        </span><span class="sh">"""</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <p><a class="anchor" id="section5"></a></p> <h2 style="color:purple;font-size: 1.5em;"> Positional Encoding</h2> \[PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\] \[PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\] <p>Here \(pos\) is the position of the word in the sentence, and \(i\) refers to position along embedding vector dimension.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">max_seq_len</span><span class="p">,</span><span class="n">embed_model_dim</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            seq_len: length of input sequence
            embed_model_dim: demension of embedding, d_model
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PositionalEmbedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_model_dim</span>

        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span><span class="mi">2</span><span class="p">):</span>
                <span class="n">pe</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)))</span>
                <span class="n">pe</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)))</span>

        <span class="c1"># adding batch dimension for broadcasting
</span>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> 

        <span class="c1"># register buffer in Pytorch -&gt;
</span>        <span class="c1"># If you have parameters in your model, which should be saved and restored in the state_dict,
</span>        <span class="c1"># but not trained by the optimizer, you should register them as buffers.
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">pe</span><span class="sh">'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            x: input vector
        Returns:
            x: output
        </span><span class="sh">"""</span>
        <span class="c1"># make embeddings relatively larger
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="c1">#add constant to embedding
</span>        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">].</span><span class="nf">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p><a class="anchor" id="section6"></a></p> <h2 style="color:purple; font-size: 1.5em;"> Self Attention</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            embed_dim: dimension of embeding vector output
            n_heads: number of self attention heads
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>    <span class="c1">#512 dim
</span>        <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>   <span class="c1">#8
</span>        <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">)</span>   <span class="c1">#512/8 = 64  . each key,query, value will be of 64d
</span>       
        <span class="c1">#key,query and value matrixes    #64 x 64   
</span>        <span class="n">self</span><span class="p">.</span><span class="n">query_matrix</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># single key matrix for all 8 keys #512x512
</span>        <span class="n">self</span><span class="p">.</span><span class="n">key_matrix</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span>  <span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value_matrix</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span> 

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">key</span><span class="p">,</span><span class="n">query</span><span class="p">,</span><span class="n">value</span><span class="p">,</span><span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>    <span class="c1">#batch_size x sequence_length x embedding_dim    # 32 x 10 x 512
</span>        
        <span class="sh">"""</span><span class="s">
        Args:
           key : key vector
           query : query vector
           value : value vector
           mask: mask for decoder
        
        Returns:
           output vector from multihead attention
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">seq_length</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># query dimension can change in decoder during inference. 
</span>        <span class="c1"># so we cant take general seq_length
</span>        <span class="n">seq_length_query</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># 32x10x512
</span>        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span>  <span class="c1">#batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)
</span>        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length_query</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span> <span class="c1">#(32x10x8x64)
</span>        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span> <span class="c1">#(32x10x8x64)
</span>       
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key_matrix</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>       <span class="c1"># (32x10x8x64)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query_matrix</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>   
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value_matrix</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)
</span>        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, n_heads, seq_len, single_head_dim)
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, n_heads, seq_len, single_head_dim)
</span>       
        <span class="c1"># computes attention
</span>        <span class="c1"># adjust key for matrix multiplication
</span>        <span class="n">k_adjusted</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1">#(batch_size, n_heads, single_head_dim, seq_ken)  #(32 x 8 x 64 x 10)
</span>        <span class="n">product</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k_adjusted</span><span class="p">)</span>  <span class="c1">#(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = #(32x8x10x10)
</span>        
        <span class="c1"># fill those positions of product matrix as (-1e20) where mask positions are 0
</span>        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
             <span class="n">product</span> <span class="o">=</span> <span class="n">product</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">"</span><span class="s">-1e20</span><span class="sh">"</span><span class="p">))</span>

        <span class="c1">#divising by square root of key dimension
</span>        <span class="n">product</span> <span class="o">=</span> <span class="n">product</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span> <span class="c1"># / sqrt(64)
</span>
        <span class="c1">#applying softmax
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">product</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
 
        <span class="c1">#mutiply with value matrix
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1">##(32x8x 10x 10) x (32 x 8 x 10 x 64) = (32 x 8 x 10 x 64) 
</span>        
        <span class="c1">#concatenated output
</span>        <span class="n">concat</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length_query</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">)</span>  <span class="c1"># (32x8x10x64) -&gt; (32x10x8x64)  -&gt; (32,10,512)
</span>        
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">out</span><span class="p">(</span><span class="n">concat</span><span class="p">)</span> <span class="c1">#(32,10,512) -&gt; (32,10,512)
</span>       
        <span class="k">return</span> <span class="n">output</span>

</code></pre></div></div> <p><a class="anchor" id="section7"></a></p> <h2 style="color:purple; font-size: 1.5em;"> Transformer Block</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="sh">"""</span><span class="s">
        Args:
           embed_dim: dimension of the embedding
           expansion_factor: fator ehich determines output dimension of linear layer
           n_heads: number of attention heads
        
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                          <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">*</span><span class="n">embed_dim</span><span class="p">),</span>
                          <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">expansion_factor</span><span class="o">*</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">key</span><span class="p">,</span><span class="n">query</span><span class="p">,</span><span class="n">value</span><span class="p">):</span>
        
        <span class="sh">"""</span><span class="s">
        Args:
           key: key vector
           query: query vector
           value: value vector
           norm2_out: output of transformer block
        
        </span><span class="sh">"""</span>
        
        <span class="n">attention_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">key</span><span class="p">,</span><span class="n">query</span><span class="p">,</span><span class="n">value</span><span class="p">)</span>  <span class="c1">#32x10x512
</span>        <span class="n">attention_residual_out</span> <span class="o">=</span> <span class="n">attention_out</span> <span class="o">+</span> <span class="n">value</span>  <span class="c1">#32x10x512
</span>        <span class="n">norm1_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout1</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">attention_residual_out</span><span class="p">))</span> <span class="c1">#32x10x512
</span>
        <span class="n">feed_fwd_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">norm1_out</span><span class="p">)</span> <span class="c1">#32x10x512 -&gt; #32x10x2048 -&gt; 32x10x512
</span>        <span class="n">feed_fwd_residual_out</span> <span class="o">=</span> <span class="n">feed_fwd_out</span> <span class="o">+</span> <span class="n">norm1_out</span> <span class="c1">#32x10x512
</span>        <span class="n">norm2_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout2</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">feed_fwd_residual_out</span><span class="p">))</span> <span class="c1">#32x10x512
</span>
        <span class="k">return</span> <span class="n">norm2_out</span>

</code></pre></div></div> <p><a class="anchor" id="section8"></a></p> <h2 style="color:purple;font-size: 2em;"> 4. Encoder</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Args:
        seq_len : length of input sequence
        embed_dim: dimension of embedding
        num_layers: number of encoder layers
        expansion_factor: factor which determines number of linear layers in feed forward layer
        n_heads: number of heads in multihead attention
        
    Returns:
        out: output of the encoder
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerEncoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">positional_encoder</span> <span class="o">=</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">embed_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">positional_encoder</span><span class="p">(</span><span class="n">embed_out</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">out</span><span class="p">,</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>  <span class="c1">#32x10x512
</span></code></pre></div></div> <p><a class="anchor" id="section9"></a></p> <h2 style="color:purple;font-size: 2em;"> 5. Decoder</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">DecoderBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="sh">"""</span><span class="s">
        Args:
           embed_dim: dimension of the embedding
           expansion_factor: fator ehich determines output dimension of linear layer
           n_heads: number of attention heads
        
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">transformer_block</span> <span class="o">=</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>       
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span><span class="n">mask</span><span class="p">):</span>
        
        <span class="sh">"""</span><span class="s">
        Args:
           key: key vector
           query: query vector
           value: value vector
           mask: mask to be given for multi head attention 
        Returns:
           out: output of transformer block
    
        </span><span class="sh">"""</span>
        <span class="c1">#we need to pass mask mask only to fst attention
</span>        <span class="n">attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span> <span class="c1">#32x10x512
</span>        <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">attention</span> <span class="o">+</span> <span class="n">x</span><span class="p">))</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer_block</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">TransformerDecoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerDecoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="sh">"""</span><span class="s">  
        Args:
           target_vocab_size: vocabulary size of taget
           embed_dim: dimension of embedding
           seq_len : length of input sequence
           num_layers: number of encoder layers
           expansion_factor: factor which determines number of linear layers in feed forward layer
           n_heads: number of heads in multihead attention
        
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">word_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">position_embedding</span> <span class="o">=</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="nc">DecoderBlock</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span> 
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_out</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        
        <span class="sh">"""</span><span class="s">
        Args:
            x: input vector from target
            enc_out : output from encoder layer
            trg_mask: mask for decoder self attention
        Returns:
            out: output vector
        </span><span class="sh">"""</span>     
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">word_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1">#32x10x512
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#32x10x512
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
     
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">enc_out</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_out</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> 

        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc_out</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">out</span>

</code></pre></div></div> <p><a class="anchor" id="section10"></a></p> <h2 style="color:purple;font-size: 2em;"> 6. Transformer</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="sh">"""</span><span class="s">  
        Args:
           embed_dim:  dimension of embedding 
           src_vocab_size: vocabulary size of source
           target_vocab_size: vocabulary size of target
           seq_length : length of input sequence
           num_layers: number of encoder layers
           expansion_factor: factor which determines number of linear layers in feed forward layer
           n_heads: number of heads in multihead attention
        
        </span><span class="sh">"""</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">target_vocab_size</span> <span class="o">=</span> <span class="n">target_vocab_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="n">expansion_factor</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="nc">TransformerDecoder</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="n">expansion_factor</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">)</span>
        
    
    <span class="k">def</span> <span class="nf">make_trg_mask</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">trg</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            trg: target sequence
        Returns:
            trg_mask: target mask
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">trg_len</span> <span class="o">=</span> <span class="n">trg</span><span class="p">.</span><span class="n">shape</span>
        <span class="c1"># returns the lower triangular part of matrix filled with ones
</span>        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">trg_len</span><span class="p">,</span> <span class="n">trg_len</span><span class="p">))).</span><span class="nf">expand</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">trg_len</span><span class="p">,</span> <span class="n">trg_len</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">trg_mask</span>    

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">src</span><span class="p">,</span><span class="n">trg</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        for inference
        Args:
            src: input to encoder 
            trg: input to decoder
        out:
            out_labels : returns final prediction of sequence
        </span><span class="sh">"""</span>
        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_trg_mask</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
        <span class="n">enc_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="n">out_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">batch_size</span><span class="p">,</span><span class="n">seq_len</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">src</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1">#outputs = torch.zeros(seq_len, batch_size, self.target_vocab_size)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">trg</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span> <span class="c1">#10
</span>            <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">enc_out</span><span class="p">,</span><span class="n">trg_mask</span><span class="p">)</span> <span class="c1">#bs x seq_len x vocab_dim
</span>            <span class="c1"># taking the last token
</span>            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>
     
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">out_labels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
          
        
        <span class="k">return</span> <span class="n">out_labels</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">trg</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            src: input to encoder 
            trg: input to decoder
        out:
            out: final vector which returns probabilities of each target word
        </span><span class="sh">"""</span>
        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_trg_mask</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
        <span class="n">enc_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
   
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">trg</span><span class="p">,</span> <span class="n">enc_out</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>


</code></pre></div></div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[based on the paper "Attention is all you need"]]></summary></entry><entry><title type="html">Improve resume using LLM</title><link href="https://19revey.github.io/blog/2023/resume/" rel="alternate" type="text/html" title="Improve resume using LLM"/><published>2023-01-02T17:39:00+00:00</published><updated>2023-01-02T17:39:00+00:00</updated><id>https://19revey.github.io/blog/2023/resume</id><content type="html" xml:base="https://19revey.github.io/blog/2023/resume/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[improve resume based on job description using Google Gemini Pro]]></summary></entry><entry><title type="html">Compare classification models</title><link href="https://19revey.github.io/blog/2022/ml_classification/" rel="alternate" type="text/html" title="Compare classification models"/><published>2022-05-01T12:57:00+00:00</published><updated>2022-05-01T12:57:00+00:00</updated><id>https://19revey.github.io/blog/2022/ml_classification</id><content type="html" xml:base="https://19revey.github.io/blog/2022/ml_classification/"><![CDATA[<h3 id="compare-different-classification-models-using-the-titanic-disaster-data">Compare different classification models using the titanic disaster data</h3> <p><br/></p> <table> <thead> <tr> <th>Model</th> <th>F1 score</th> </tr> </thead> <tbody> <tr> <td>Random Forest</td> <td>0.7917</td> </tr> <tr> <td>Logistic Regression</td> <td>0.7586</td> </tr> <tr> <td>Neural Network</td> <td>0.7536</td> </tr> <tr> <td>K-Neighbors</td> <td>0.7500</td> </tr> <tr> <td>Naive Bayes</td> <td>0.7436</td> </tr> <tr> <td>XGBClassifier</td> <td>0.7347</td> </tr> <tr> <td>Decision Tree</td> <td>0.7020</td> </tr> </tbody> </table> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/ml_compare.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[based on the titanic disaster data]]></summary></entry><entry><title type="html">Batch and layer normalization</title><link href="https://19revey.github.io/blog/2022/ml_components/" rel="alternate" type="text/html" title="Batch and layer normalization"/><published>2022-03-04T22:20:00+00:00</published><updated>2022-03-04T22:20:00+00:00</updated><id>https://19revey.github.io/blog/2022/ml_components</id><content type="html" xml:base="https://19revey.github.io/blog/2022/ml_components/"><![CDATA[<h2 style="color:purple;font-size: 2em;">Overview</h2> <ul> <li><a href="#section1">Batch normalization</a></li> <li><a href="#section2">Layer normalization</a></li> </ul> <h5 id="understand-normalization-in-a-neural-network">Understand normalization in a neural network</h5> <ul> <li><a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li> <li><a href="https://arxiv.org/pdf/1607.06450.pdf">Layer Normalization</a></li> <li><a href="https://www.pinecone.io/learn/batch-layer-normalization/">https://www.pinecone.io/learn/batch-layer-normalization/</a></li> </ul> <p><a class="anchor" id="section1"></a></p> <h2 style="color:purple;font-size: 2em;">Batch and layer normalization</h2> <table> <thead> <tr> <th> </th> <th>batch normalization</th> <th>layer normalization</th> </tr> </thead> <tbody> <tr> <td>trainable parameter</td> <td>2*num_feature</td> <td>2</td> </tr> <tr> <td>normalization across</td> <td>num_batch</td> <td>num_feature</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Docker</title><link href="https://19revey.github.io/blog/2022/docker/" rel="alternate" type="text/html" title="Docker"/><published>2022-03-01T08:13:00+00:00</published><updated>2022-03-01T08:13:00+00:00</updated><id>https://19revey.github.io/blog/2022/docker</id><content type="html" xml:base="https://19revey.github.io/blog/2022/docker/"><![CDATA[<p><br/></p> <h3 id="install-docker">Install Docker</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get update <span class="nt">-y</span>
<span class="nb">sudo </span>apt-get upgrade <span class="nt">-y</span>

curl <span class="nt">-fsSL</span> https://get.docker.com <span class="nt">-o</span> get-docker.sh

<span class="nb">sudo </span>sh get-docker.sh <span class="nt">-y</span>

<span class="nb">sudo </span>usermod <span class="nt">-aG</span> docker ubuntu 

newgrp docker
</code></pre></div></div> <p><br/></p> <h3 id="nvidia-container-toolkit"><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">Nvidia Container Toolkit</a></h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-fsSL</span> https://nvidia.github.io/libnvidia-container/gpgkey | <span class="nb">sudo </span>gpg <span class="nt">--dearmor</span> <span class="nt">-o</span> /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg <span class="se">\</span>
  <span class="o">&amp;&amp;</span> curl <span class="nt">-s</span> <span class="nt">-L</span> https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | <span class="se">\</span>
    <span class="nb">sed</span> <span class="s1">'s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g'</span> | <span class="se">\</span>
    <span class="nb">sudo tee</span> /etc/apt/sources.list.d/nvidia-container-toolkit.list

<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> nvidia-container-toolkit

<span class="nb">sudo </span>nvidia-ctk runtime configure <span class="nt">--runtime</span><span class="o">=</span>docker

<span class="nb">sudo </span>systemctl restart docker
</code></pre></div></div> <p><br/></p> <h3 id="dockerfile-example">Dockerfile example</h3> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Use a base image</span>
<span class="c">#FROM python:3.11-slim</span>
<span class="k">FROM</span><span class="s"> nvidia/cuda:12.1.0-base-ubuntu22.04</span>
<span class="c"># Set environment variables</span>
<span class="k">ENV</span><span class="s"> MY_VARIABLE=my_value</span>
<span class="c"># Install dependencies</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="se">\
</span>    build-essential <span class="se">\
</span>    software-properties-common <span class="se">\
</span>    git <span class="se">\
</span>    python3-pip <span class="se">\
</span>    <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/apt/lists/<span class="k">*</span>
<span class="c"># Set the working directory</span>
<span class="k">WORKDIR</span><span class="s"> /app</span>
<span class="c"># Copy files into the image</span>
<span class="k">COPY</span><span class="s"> . /app</span>
<span class="c"># Expose a port</span>
<span class="k">EXPOSE</span><span class="s"> 8051</span>
<span class="c"># Install Python dependencies based on the requirements.txt file</span>
<span class="k">RUN </span>pip3 <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
<span class="c"># Configure a container that will run as an executable</span>
<span class="k">ENTRYPOINT</span><span class="s"> ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]</span>
<span class="c"># Define a command to run on container startup</span>
<span class="k">CMD</span><span class="s"> ["./start.sh"]</span>
<span class="c"># Keep container running</span>
<span class="k">CMD</span><span class="s"> ["tail -f /dev/null"]</span>
</code></pre></div></div> <table> <tbody> <tr> <td>Build image</td> <td><code class="language-plaintext highlighter-rouge">docker build -t image_name .</code></td> </tr> <tr> <td>Run container</td> <td><code class="language-plaintext highlighter-rouge">docker run -b -p 8501:8501 -e API="XXX" --name image_container image_name</code></td> </tr> <tr> <td>Remove all container.</td> <td><code class="language-plaintext highlighter-rouge">docker rm $(docker ps -a -q)</code></td> </tr> <tr> <td>Remove all iamges.</td> <td><code class="language-plaintext highlighter-rouge">docker image rm $(docker images -q)</code></td> </tr> </tbody> </table> <p><br/></p> <h3 id="the-process-of-building-and-running-docker-containers-can-be-automated-using-tools-like-docker-compose">The process of building and running Docker containers can be automated using tools like Docker Compose.</h3> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code>services:
  web:  #service name
    build: .
    shm_size: 8gb <span class="c"># increase shared memory</span>
    ports:
      - "8501:8501"
    volumes:
      - .:/app
    environment:
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - NVIDIA_VISIBLE_DEVICES=all
    command: tail -f /dev/null  <span class="c">#keep container running in the background</span>
</code></pre></div></div> <table> <tbody> <tr> <td>Build image and run container</td> <td><code class="language-plaintext highlighter-rouge">docker compose up --detach</code></td> </tr> <tr> <td>remove container</td> <td><code class="language-plaintext highlighter-rouge">docker compose down</code></td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="computer_science"/><summary type="html"><![CDATA[some docker commands]]></summary></entry><entry><title type="html">Git &amp;amp; Github</title><link href="https://19revey.github.io/blog/2022/git/" rel="alternate" type="text/html" title="Git &amp;amp; Github"/><published>2022-01-21T22:13:00+00:00</published><updated>2022-01-21T22:13:00+00:00</updated><id>https://19revey.github.io/blog/2022/git</id><content type="html" xml:base="https://19revey.github.io/blog/2022/git/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_git/gitworkflow-480.webp 480w,/assets/img/blog_git/gitworkflow-800.webp 800w,/assets/img/blog_git/gitworkflow-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_git/gitworkflow.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="tell-git-who-you-are">Tell Git who you are</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>Configure the author name.</td> <td><code class="language-plaintext highlighter-rouge">git config --global user.name "&lt;username&gt;"</code></td> </tr> <tr> <td>Configure the author email address.</td> <td><code class="language-plaintext highlighter-rouge">git config --global user.email &lt;email address&gt;</code></td> </tr> </tbody> </table> <h3 id="getting--creating-projects">Getting &amp; Creating Projects</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>Initialize a local Git repository</td> <td><code class="language-plaintext highlighter-rouge">git init</code></td> </tr> <tr> <td>Create a local copy of a remote repository</td> <td><code class="language-plaintext highlighter-rouge">git clone ssh://git@github.com/&lt;username&gt;/&lt;repository-name&gt;.git</code></td> </tr> </tbody> </table> <h3 id="basic-snapshotting">Basic Snapshotting</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>Check status</td> <td><code class="language-plaintext highlighter-rouge">git status</code></td> </tr> <tr> <td>Add a file to the staging area</td> <td><code class="language-plaintext highlighter-rouge">git add &lt;file-name.txt&gt;</code></td> </tr> <tr> <td>Add all new and changed files to the staging area</td> <td><code class="language-plaintext highlighter-rouge">git add -A</code> or <br/> <code class="language-plaintext highlighter-rouge">git add .</code></td> </tr> <tr> <td>Commit changes</td> <td><code class="language-plaintext highlighter-rouge">git commit -m "&lt;commit message&gt;"</code></td> </tr> <tr> <td>Remove a file (or folder)</td> <td><code class="language-plaintext highlighter-rouge">git rm -r &lt;file-name.txt&gt;</code></td> </tr> </tbody> </table> <h3 id="inspection--comparison">Inspection &amp; Comparison</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>View changes</td> <td><code class="language-plaintext highlighter-rouge">git log</code></td> </tr> <tr> <td>View changes (detailed)</td> <td><code class="language-plaintext highlighter-rouge">git log --summary</code></td> </tr> <tr> <td>View changes in one line (briefly)</td> <td><code class="language-plaintext highlighter-rouge">git log --oneline</code> or <br/> <code class="language-plaintext highlighter-rouge">git log --pretty=oneline</code> or<br/> <code class="language-plaintext highlighter-rouge">git log --pretty=short</code></td> </tr> </tbody> </table> <h3 id="undo-to-previous-file">Undo to previous file</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>List of all commit with commit id and commit message)</td> <td><code class="language-plaintext highlighter-rouge">git log --oneline</code></td> </tr> <tr> <td>Return to previous commit <commit></commit></td> <td><code class="language-plaintext highlighter-rouge">git checkout&lt;commit id&gt;</code></td> </tr> <tr> <td>Revert commit <commit> (undo one particular commit)</commit></td> <td><code class="language-plaintext highlighter-rouge">git revert &lt;commit id&gt;</code></td> </tr> <tr> <td>Reset to previous commit <commit> (remove history of all commit after <commit> )</commit></commit></td> <td><code class="language-plaintext highlighter-rouge">git reset --hard &lt;commit id&gt;</code></td> </tr> <tr> <td>Stop a file being tracked</td> <td><code class="language-plaintext highlighter-rouge">git rm --cached &lt;file/folder&gt;</code></td> </tr> <tr> <td>Restore a file to a previous commit</td> <td><code class="language-plaintext highlighter-rouge">git checkout &lt;file/to/restore&gt;</code></td> </tr> </tbody> </table> <h3 id="branching--merging">Branching &amp; Merging</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>List branches (the asterisk denotes the current branch)</td> <td><code class="language-plaintext highlighter-rouge">git branch</code></td> </tr> <tr> <td>List all branches (local and remote)</td> <td><code class="language-plaintext highlighter-rouge">git branch -a</code></td> </tr> <tr> <td>Create a new branch</td> <td><code class="language-plaintext highlighter-rouge">git branch &lt;branch name&gt;</code></td> </tr> <tr> <td>Create a new branch and switch to it</td> <td><code class="language-plaintext highlighter-rouge">git checkout -b &lt;branch name&gt;</code></td> </tr> <tr> <td>Clone a remote branch and switch to it</td> <td><code class="language-plaintext highlighter-rouge">git checkout -b &lt;branch name&gt; origin/&lt;branch name&gt;</code></td> </tr> <tr> <td>Rename a local branch</td> <td><code class="language-plaintext highlighter-rouge">git branch -m &lt;old branch name&gt; &lt;new branch name&gt;</code></td> </tr> <tr> <td>Switch to a branch</td> <td><code class="language-plaintext highlighter-rouge">git checkout &lt;branch name&gt;</code></td> </tr> <tr> <td>Switch to the branch last checked out</td> <td><code class="language-plaintext highlighter-rouge">git checkout -</code></td> </tr> <tr> <td>Discard changes to a file</td> <td><code class="language-plaintext highlighter-rouge">git checkout -- &lt;file-name.txt&gt;</code></td> </tr> <tr> <td>Delete a branch</td> <td><code class="language-plaintext highlighter-rouge">git branch -d &lt;branch name&gt;</code></td> </tr> <tr> <td>Delete a remote branch</td> <td><code class="language-plaintext highlighter-rouge">git push origin --delete &lt;branch name&gt;</code></td> </tr> <tr> <td>Preview changes before merging</td> <td><code class="language-plaintext highlighter-rouge">git diff &lt;source branch&gt; &lt;target branch&gt;</code></td> </tr> <tr> <td>Merge a branch into the active branch</td> <td><code class="language-plaintext highlighter-rouge">git merge &lt;branch name&gt;</code></td> </tr> <tr> <td>Merge a branch into a target branch</td> <td><code class="language-plaintext highlighter-rouge">git merge &lt;source branch&gt; &lt;target branch&gt;</code></td> </tr> <tr> <td>Stash changes in a dirty working directory</td> <td><code class="language-plaintext highlighter-rouge">git stash</code></td> </tr> <tr> <td>Remove all stashed entries</td> <td><code class="language-plaintext highlighter-rouge">git stash clear</code></td> </tr> </tbody> </table> <h3 id="sharing--updating-projects">Sharing &amp; Updating Projects</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>Push a branch to your remote repository</td> <td><code class="language-plaintext highlighter-rouge">git push origin &lt;branch name&gt;</code></td> </tr> <tr> <td>Push changes to remote repository (and remember the branch)</td> <td><code class="language-plaintext highlighter-rouge">git push -u origin &lt;branch name&gt;</code></td> </tr> <tr> <td>Push changes to remote repository (remembered branch)</td> <td><code class="language-plaintext highlighter-rouge">git push</code></td> </tr> <tr> <td>Push changes to remote repository all branch</td> <td><code class="language-plaintext highlighter-rouge">git push --all</code></td> </tr> <tr> <td>Push changes to remote repository (Force)</td> <td><code class="language-plaintext highlighter-rouge">git push -f</code></td> </tr> <tr> <td>Delete a remote branch</td> <td><code class="language-plaintext highlighter-rouge">git push origin --delete &lt;branch name&gt;</code></td> </tr> <tr> <td>Update local repository to the newest commit</td> <td><code class="language-plaintext highlighter-rouge">git pull</code></td> </tr> <tr> <td>Pull changes from remote repository</td> <td><code class="language-plaintext highlighter-rouge">git pull origin &lt;branch name&gt;</code></td> </tr> <tr> <td>Add a remote repository</td> <td><code class="language-plaintext highlighter-rouge">git remote add origin ssh://git@github.com/&lt;username&gt;/&lt;repository-name&gt;.git</code></td> </tr> <tr> <td>Set a repository’s origin branch to SSH</td> <td><code class="language-plaintext highlighter-rouge">git remote set-url origin ssh://git@github.com/&lt;username&gt;/&lt;repository-name&gt;.git</code></td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="computer_science"/><summary type="html"><![CDATA[some git commands]]></summary></entry></feed>